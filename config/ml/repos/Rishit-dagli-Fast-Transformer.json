{
  "expertise": "INTERMEDIATE",
  "version": 1,
  "source": "github",
  "authorIds": [
    "rishit-dagli"
  ],
  "owner": "Rishit-dagli",
  "repo": "Fast-Transformer",
  "name": "Fast Transformer",
  "shortDescription": "This repo implements Fastformer: Additive Attention Can Be All You Need by Wu et al. in TensorFlow. Fast Transformer is a Transformer variant based on additive attention that can handle long sequences efficiently with linear complexity. Fastformer is much more efficient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance.",
  "longDescription": "This repo implements Fastformer: Additive Attention Can Be All You Need by Wu et al. in TensorFlow. Fast Transformer is a Transformer variant based on additive attention that can handle long sequences efficiently with linear complexity. Fastformer is much more efficient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance.",
  "content": "README.md",
  "pages": [],
  "tags": [
    "mobile",
    "nlp",
    "vision",
    "library"
  ]
}
